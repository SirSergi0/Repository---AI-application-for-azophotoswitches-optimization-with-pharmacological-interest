\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=1.5cm, bottom=3.5cm, left=2.2cm, right=2.2cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd, esint}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage[table,xcdraw]{xcolor}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{yfonts}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage[normalem]{ulem}
\usepackage{multicol}
\useunder{\uline}{\ul}{}

\newtheorem{definition}{Definition}
\newtheorem{observation}{Observation}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

% Edit these as appropriate
\newcommand\course{}
\newcommand\subject{Final Degree Project}
\newcommand\degree{Bachelor's Degree in Chemistry}
\newcommand\documenttitle{Notes: AI application for azophotoswitches' optimization with pharmacological interest}
\newcommand\NetIDb{Universitat Autònoma de Barcelona}


\begin{document}
\title{\vspace{4cm} \normalsize 
		\includegraphics[width = 0.25\textwidth]{GeneralSources/UABLogo.png}\\ [0.5cm]
		\textsc{\NetIDb}\\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{\documenttitle}}
		\HRule{2pt} \\ [1.5cm]
		\normalsize \begin{tabular}{rcl}  % Create a right-left column alignment
        \textsc{Author} & : & \textsc{Sergio Castañeiras Morales} \\
        \textsc{Supervisor} & : & \textsc{Miquel Moreno Ferrer} \\
        \textsc{Co-Supervisor} & : & \textsc{Àngels Gonzalez Lafont}
    \end{tabular}
    \normalsize \vspace*{5\baselineskip}
		}

\date{2024-2025}

\author{\large \textsc{\subject} \\ \textsc{\degree}}



\begin{titlepage}
\clearpage\maketitle
\thispagestyle{empty}
\end{titlepage}


\newpage
\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDb}    
\rhead{\subject}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em
\begin{multicols}{2}
[
\section{Definitions}
]
\begin{definition}\label{definitionIC50}
$IC_{50}$: Half maximal inhibitory concentration "$IC_{50}$ is the concentration of drug required for $50\%$ inhibition. $IC_{50}$ is an operational term dependent on the assay conditions. $IC_{90}$ or $IC_{99}$ is sometimes used when complete inhibition is required. Calculation of the fractional occupancy shows that $IC_{90}$ concentration is approximately 10-fold greater than the $IC_{50}$ concentration assuming one-site binding at equilibrium with a Hill coefficient of 1."\cite{BookIC50}\\
 For this project, we aim to identify substances with the lowest possible $IC_{50}$, as our goal is to minimize the presence of foreign substances in the living organism.
\end{definition}
\begin{definition}
Molecular descriptor: "A molecular descriptor is the final result of a logical and mathematical procedure which transforms chemical information encoded within a symbolic representation of a molecule into a useful number or the result of some standardized experiment."\cite{DescriptorsBook}
\end{definition}
\end{multicols}
\newpage
\begin{multicols}{2}
[
\section{Observations}
]
\begin{observation}
Machine Leaning (ML) and Artificial Intelligence (AI) are not the same concept, in fact, a ML  models are a subset of AIs. The key relies in word \emph{Learning}, generally speaking, an AI does not need to actually learn from a set of data. It can be set up within a decision tree such that logically responses with the proper answer following its criteria. \par
However a Machine Leaning Method is a kind of AI that \emph{learns} from data and evolves with the provided data.
\end{observation}
\begin{observation}
Mathematically speaking, Machine Learning methods are no different from an optimisation problem \cite{MathematicsForMachineLearningBook}.\par
Generally speaking, all ML methods seek to find a criterion to determine the quantity of a certain property of some data, which we lack knowledge of and will call the \emph{target}. In order to do so, they rely on a dataset where the sought property is already known and which establishes relationships between this property and other features of the data. Then, based on these relationships and the properties of the \emph{target}, the model searches for the position where the \emph{target} fits within the data and determines the quantity of the property, taking this placement into account within the known data.\par
For instance, we could think of standard calibration with linear regression as an ultra-simplification of a Machine Learning problem. We have a set of data (such as salt concentration in a solution) $\{x_i\}_{i=0}^n$, where we know the quantity of a certain property (such as the solution's conductivity) for each entry $x_j$, denoted as $\{f(x_i)\}_{i=0}^n$. In this example, our ML method assumes that the data and the property are linearly related and that the distance between data and the property is equivalently defined as $d(x_i, x_j) = \sqrt{x_i^2 + x_j^2}$. Hence, based on the assumption of linearity, the best possible relationship is a straight line, and the method looks for the line that minimises the distance between the line and the data.\par
Then, given a \emph{target} (for instance, a certain concentration), about which we initially lack knowledge, the method is capable of quantitatively computing the theoretical property (for instance, its conductivity) by using the fitted line representing the relationship. Consequently, this can be helpful in determining the theoretical concentration of salt needed to achieve a certain conductivity without having actual data for that concentration.\par
The \emph{learning} part (the most important part) comes from the fact that the model (the line) learns from the data. As we train our system (i.e., perform linear regression) with increasingly larger datasets, the accuracy of our model's predictions improves. Thus, we can conclude that the system is \emph{learning} from the data, which qualifies it as a Machine Learning model.\par
\end{observation}
\begin{observation}
Since we are working with molecules, our Machine Learning model will require certain parameters to determine the relationship with the property.\par
In this case, our \emph{property} (i.e., $\{f(x_i)\}_{i=0}^n$) will be the inhibition of a specific protein (COX-2), and more specifically, it will be $IC_{50}$, $IC_{90}$, or $IC_{99}$ (\ref{definitionIC50}).\par
Moreover, our \emph{parameters} (i.e., $\{x_i\}_{i=0}^n$) will be the molecular descriptors, which must be \textbf{carefully chosen}. This problem introduces additional complexities compared to the linear regression case:
\begin{itemize}
    \item We will be working with approximately $10^3$ descriptors, making this a $10^3$-dimensional problem.
    \item We want the system to automatically discard non-relevant data, as some molecules could be exceptions due to external factors.
    \item We do not assume linearity.
    \item Not all descriptors have the same distances or weights, as we will want some of them to be more relevant than others.
\end{itemize}
Interestingly enough, despite these complexities, in some way, we can say that this project is not entirely different from computing a linear regression...
\end{observation}
\end{multicols}




\newpage
%Bibliography %
\bibliographystyle{plain}
\bibliography{references} 
%%%%%%%%


\end{document}